#!/usr/bin/env python

# Reads an OONI http_requests report and shows URLs that have known block pages.
#
# First, make an OONI report:
#   ooniprobe -i /usr/share/ooni/decks/complete_no_root.deck
# Then,
#   ./findblocks report-http_requests-XXXX.yamloo

# Should look for timeout blocks.
# http://www.freedomhouse.org: control_failure=task_timed_out

import csv
import datetime
import errno
import getopt
import os
import os.path
import re
import subprocess
import sys
import urllib
import urlparse
import yaml

from classify import classify_response

class options(object):
    save_blocks_dir = None

def usage(f=sys.stdout):
    f.write("""\
Usage: %s *-http_requests-v1-probe.yaml.gz

  --save-blocks=DIR  save block pages to directory DIR.
  -h, --help         show this help.
""" % sys.argv[0])

# Return a (nontor, tor) pair if there are exactly two requests and one is
# nontor and one is tor, or else raise an exception.
def split_requests(requests):
    nontor = None
    tor = None
    for request in requests:
        if request.get("failure") is not None:
            continue
        if not request["request"]["tor"]["is_tor"]:
            if nontor is not None:
                raise ValueError("more than one is_tor:false request")
            nontor = request
        else:
            if tor is not None:
                raise ValueError("more than one is_tor:true request")
            tor = request
    return nontor, tor

def save_response(f, response):
    f.write("HTTP/1.0 %d xxx\r\n" % response["code"])
    for name, values in response["headers"]:
        for value in values:
            f.write("%s: %s\r\n" % (name, value))
    f.write("\r\n")
    # YAMLOO can give us either a binary string (type str) or a Unicode string
    # (type unicode, with unspecified original encoding) for the body. If it's
    # Unicode, guess the encoding was UTF-8. It might be possible to scrape the
    # Content-Type for the encoding.
    body = response["body"]
    if body is None:
        body = ""
    elif type(body) == unicode:
        body = body.encode("utf-8")
    f.write(body)

def save_response_filename(filename, response):
    try:
        os.makedirs(os.path.dirname(filename))
    except OSError, e:
        if e.errno != errno.EEXIST:
            raise
    with open(filename, "w") as f:
        return save_response(f, response)

# Save a response with an automatically generated filename.
def save_response_filename_auto(header, request, response, class_):
    url = urlparse.urlsplit(request["url"])
    filename = "%s-%s-%s%s" % (header["report_id"], url.scheme, url.netloc, urlparse.urlunsplit((None, None, url.path, url.query, None)))
    filename = urllib.quote(filename, safe="")
    # Truncate name to filesystem limits.
    filename = filename[:255]
    filename = os.path.join(options.save_blocks_dir, class_, filename)
    return save_response_filename(filename, response)

# Canonicalize paths of "/" and "", which otherwise cause spurious URL
# differences.
def canonicalize_url(url):
    u = urlparse.urlsplit(url)
    if u.path == "/":
        u = (u.scheme, u.netloc, "", u.query, u.fragment)
    return urlparse.urlunsplit(u)

def boolf(v):
    if v:
        return "T"
    else:
        return "F"

def yaml_load_sloppy(f):
    # This is much faster than yaml.safe_load_all.
    y = yaml.load_all(f, Loader=yaml.CSafeLoader)
    while True:
        while True:
            try:
                doc = next(y)
                break
            except StopIteration:
                return
            except yaml.scanner.ScannerError, e:
                # Skip ahead to the next line.
                while True:
                    line = f.readline()
                    if not line or line == "...\n":
                        break
                print >> sys.stderr, "%s, skipping ahead" % e
                y = yaml.load_all(f, Loader=yaml.CSafeLoader)
        yield doc

def process_file(f, filename):
    yamloo = yaml_load_sloppy(f)

    header = None
    for doc in yamloo:
        if doc["record_type"] == "header":
            assert header is None
            header = doc
            continue

        if doc["record_type"] != "entry":
            continue

        assert header is not None

        if options.save_blocks_dir is not None:
            for request in doc["requests"]:
                try:
                    isblocked, class_ = classify_response(request["response"])
                except KeyError, e:
                    # No response.
                    continue
                if isblocked:
                    save_response_filename_auto(header, request["request"], request["response"], class_)

        nontor, tor = split_requests(doc["requests"])
        if nontor is None:
            print >> sys.stderr, "%s %s: missing is_tor:false request: %s" % (filename, doc["input"], doc["experiment_failure"])
        if tor is None:
            print >> sys.stderr, "%s %s: missing is_tor:true request: %s" % (filename, doc["input"], doc["control_failure"])
        if nontor is None or tor is None:
            continue

        nontor_isblocked, nontor_class = classify_response(nontor["response"])
        tor_isblocked, tor_class = classify_response(tor["response"])

        try:
            test_date = datetime.datetime.utcfromtimestamp(doc["test_start_time"])
        except KeyError:
            # Some reports don't have individual test_start_time.
            test_date = datetime.datetime.utcfromtimestamp(header["start_time"])
        csvw.writerow({
            "date": test_date.strftime("%Y-%m-%d %H:%M:%S"),
            "report_id": header["report_id"],
            "probe_cc": header["probe_cc"],
            "url": canonicalize_url(doc["input"]),
            "nontor_isblocked": boolf(nontor_isblocked),
            "nontor_status": nontor["response"]["code"],
            "nontor_class": nontor_class,
            "tor_isblocked": boolf(tor_isblocked),
            "tor_status": tor["response"]["code"],
            # Sometimes exit_ip and exit_name are missing.
            "tor_exit_ip": tor["request"]["tor"].get("exit_ip", ""),
            "tor_exit_nickname": tor["request"]["tor"].get("exit_name", ""),
            "tor_class": tor_class,
        })
        sys.stdout.flush()

# Wow! Python's gzip module is slow! Uncompress with gzip instead. (Also has the
# advantage that decompression can use a separate core.)
def open_zcat(filename):
    p = subprocess.Popen(["gzip", "-dc", filename], stdout=subprocess.PIPE)
    return p.stdout

# Open a file optionally with gzip decompression based on the filename
# extension.
def magic_open(filename):
    if filename.endswith(".gz"):
        return open_zcat(filename)
    return open(filename)

# Returns (date, asn, test_name) typle.
def parse_report_filename(filename):
    m = re.match(ur'^([0-9]{8}T[0-9]{6}Z)-(AS.*)-(.*)-v1-probe.yaml', os.path.basename(filename))
    if not m:
        raise ValueError("cannot parse report filename %r" % filename)
    date = datetime.datetime.strptime(m.group(1), "%Y%m%dT%H%M%SZ")
    return date, m.group(2), m.group(3)

def process_filename(filename):
    with magic_open(filename) as f:
        return process_file(f, filename)

if __name__ == "__main__":
    opts, args = getopt.gnu_getopt(sys.argv[1:], "h", ["save-blocks=", "help"])
    for o, a in opts:
        if o == "--save-blocks":
            options.save_blocks_dir = a
        elif o == "-h" or o == "--help":
            usage()
            sys.exit()

    csvw = csv.DictWriter(sys.stdout, fieldnames=["date", "report_id", "probe_cc", "nontor_isblocked", "nontor_status", "nontor_class", "tor_isblocked", "tor_status", "tor_class", "tor_exit_ip", "tor_exit_nickname", "url"], lineterminator="\n")
    csvw.writeheader()

    for filename in args:
        process_filename(filename)
